import os
import xml.etree.ElementTree as ET
import shutil
from PIL import Image
import cv2
import numpy as np
from pathlib import Path
import glob

def resize_image_with_padding(image, target_size):
    """
    Resize image to target size while maintaining aspect ratio and padding with black.
    Works for both upscaling (small images) and downscaling (large images).
    Returns resized image and transformation parameters.
    """
    target_width, target_height = target_size
    orig_height, orig_width = image.shape[:2]
    
    # Calculate scale factor to fit image within target size
    scale = min(target_width / orig_width, target_height / orig_height)
    
    # Calculate new dimensions
    new_width = int(orig_width * scale)
    new_height = int(orig_height * scale)
    
    # Resize image (works for both upscaling and downscaling)
    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
    
    # Create padded image with black background
    padded = np.zeros((target_height, target_width, 3), dtype=np.uint8)
    
    # Calculate padding offsets (center the image)
    x_offset = (target_width - new_width) // 2
    y_offset = (target_height - new_height) // 2
    
    # Place resized image in center
    padded[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized
    
    # Return padded image and transformation info
    transform_info = {
        'scale': scale,
        'x_offset': x_offset,
        'y_offset': y_offset,
        'new_width': new_width,
        'new_height': new_height
    }
    
    return padded, transform_info

def transform_yolo_annotations(annotations, transform_info, target_size):
    """
    Transform YOLO annotations to account for padding and resizing.
    Input annotations are already normalized [0,1] based on original image.
    """
    transformed_annotations = []
    target_width, target_height = target_size
    
    for ann in annotations:
        class_id, x_center, y_center, width, height = ann[:5]
        
        # Convert normalized coordinates to pixel coordinates in resized image
        x_pixel = x_center * transform_info['new_width']
        y_pixel = y_center * transform_info['new_height']
        w_pixel = width * transform_info['new_width']
        h_pixel = height * transform_info['new_height']
        
        # Add padding offset
        x_pixel += transform_info['x_offset']
        y_pixel += transform_info['y_offset']
        
        # Convert back to normalized coordinates for target size
        x_norm = x_pixel / target_width
        y_norm = y_pixel / target_height
        w_norm = w_pixel / target_width
        h_norm = h_pixel / target_height
        
        # Ensure coordinates are within bounds
        x_norm = max(0.0, min(1.0, x_norm))
        y_norm = max(0.0, min(1.0, y_norm))
        w_norm = max(0.001, min(1.0, w_norm))
        h_norm = max(0.001, min(1.0, h_norm))
        
        transformed_annotations.append([class_id, x_norm, y_norm, w_norm, h_norm])
    
    return transformed_annotations

def get_ir_det_class_mapping():
    """Map IR_det classes to our unified YOLO classes"""
    ir_det_to_yolo_classes = {
        'person': 0,
        'people': 0,
        'pedestrian': 0,
        'human': 0,
        'bicycle': 1,
        'bike': 1,
        'car': 2,
        'vehicle': 2,
        'auto': 2,
        'motorcycle': 3,
        'motorbike': 3,
        'bus': 4,
        'truck': 5,
        'van': 5,
        'dog': 6,
        'animal': 6,
    }
    
    return ir_det_to_yolo_classes

def find_ir_det_annotation(image_filename, ir_det_base_path):
    """Find corresponding annotation file for an image"""
    base_name = os.path.splitext(image_filename)[0]
    
    # Prioritize TXT files over XML (XML files contain metadata, not annotations)
    annotation_paths = [
        os.path.join(ir_det_base_path, 'labels', 'fir', base_name + '.txt'),  # Primary FIR labels
        os.path.join(ir_det_base_path, 'converted_labels', base_name + '.txt'),  # Converted labels
        os.path.join(ir_det_base_path, 'labels', 'rgb', base_name + '.txt'),   # RGB labels backup
    ]
    
    for ann_path in annotation_paths:
        if os.path.exists(ann_path):
            return ann_path
    
    return None

def detect_annotation_format(annotation_file, original_width, original_height):
    """
    Detect if annotations are already in YOLO format (0-1 normalized) 
    or need conversion from pixel coordinates
    """
    try:
        with open(annotation_file, 'r') as f:
            lines = f.readlines()
        
        if not lines:
            return 'empty', []
        
        # Sample first few lines to detect format
        sample_coords = []
        for line in lines[:3]:  # Check first 3 lines
            parts = line.strip().split()
            if len(parts) >= 5:
                try:
                    coords = [float(x) for x in parts[1:5]]  # x, y, w, h
                    sample_coords.extend(coords)
                except ValueError:
                    continue
        
        if not sample_coords:
            return 'invalid', []
        
        # Check coordinate ranges to determine format
        max_coord = max(sample_coords)
        min_coord = min(sample_coords)
        
        # If all coordinates are between 0 and 1, likely already YOLO format
        if max_coord <= 1.0 and min_coord >= 0.0:
            return 'yolo_normalized', sample_coords
        
        # If coordinates are larger than image dimensions, likely pixel coordinates
        elif max_coord > max(original_width, original_height):
            return 'pixel_absolute', sample_coords
        
        # If coordinates are within image dimensions, could be pixel coordinates
        elif max_coord <= max(original_width, original_height) and max_coord > 1.0:
            return 'pixel_relative', sample_coords
        
        else:
            return 'unknown', sample_coords
            
    except Exception as e:
        print(f"Error detecting format for {annotation_file}: {e}")
        return 'error', []

def parse_ir_det_annotations_CORRECTED(annotation_file, original_width, original_height, class_mapping):
    """
    CORRECTED: Parse IR_det annotations with proper format detection and conversion
    """
    annotations = []
    
    # First, detect the annotation format
    format_type, sample_coords = detect_annotation_format(annotation_file, original_width, original_height)
    
    try:
        with open(annotation_file, 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            parts = line.split()
            if len(parts) >= 5:
                try:
                    # Parse class and coordinates
                    class_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    
                    # Convert coordinates based on detected format
                    if format_type == 'yolo_normalized':
                        # Already normalized - use as-is but validate
                        if (0 <= x_center <= 1 and 0 <= y_center <= 1 and 
                            0 < width <= 1 and 0 < height <= 1):
                            # Map class to our unified system (assume class_id maps directly)
                            unified_class_id = min(class_id, 6)  # Clamp to our available classes
                            annotations.append([unified_class_id, x_center, y_center, width, height])
                    
                    elif format_type in ['pixel_absolute', 'pixel_relative']:
                        # Convert from pixel coordinates to normalized
                        # Assume these are center coordinates in pixels
                        x_center_norm = x_center / original_width
                        y_center_norm = y_center / original_height
                        width_norm = width / original_width
                        height_norm = height / original_height
                        
                        # Validate (don't clamp here - let transform handle it)
                        if (0 <= x_center_norm <= 1 and 0 <= y_center_norm <= 1 and 
                            0 < width_norm <= 1 and 0 < height_norm <= 1):
                            unified_class_id = min(class_id, 6)  # Clamp to our available classes
                            annotations.append([unified_class_id, x_center_norm, y_center_norm, width_norm, height_norm])
                    
                    else:
                        # Unknown format - try to use as-is if valid
                        if (0 <= x_center <= 1 and 0 <= y_center <= 1 and 
                            0 < width <= 1 and 0 < height <= 1):
                            unified_class_id = min(class_id, 6)
                            annotations.append([unified_class_id, x_center, y_center, width, height])
                
                except ValueError:
                    continue
    
    except Exception as e:
        print(f"Warning: Error parsing annotation file {annotation_file}: {e}")
    
    return annotations, format_type

def process_ir_det_data_CORRECTED(ir_det_base_path, output_base_path, target_size=(640, 512)):
    """CORRECTED: Process IR_det data with proper annotation handling and aspect ratio preservation"""
    
    print("Processing IR_det dataset...")
    
    # Use FIR images (closest to FLIR ADK)
    fir_image_paths = [
        os.path.join(ir_det_base_path, 'Images', 'fir'),
        os.path.join(ir_det_base_path, 'ConvertedImages', 'finf'),  # FIR converted
    ]
    
    # Find the best FIR folder
    fir_folder = None
    for path in fir_image_paths:
        if os.path.exists(path):
            images = [f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            if len(images) > 0:
                fir_folder = path
                print(f"Using FIR images from: {fir_folder}")
                print(f"Found {len(images)} FIR images")
                break
    
    if fir_folder is None:
        print("Warning: No FIR images found")
        return
    
    # Get class mapping
    ir_det_to_yolo_classes = get_ir_det_class_mapping()
    
    # Output paths - add to existing main_dataset
    # Split data roughly 80/20 between train and val
    train_images_path = os.path.join(output_base_path, 'main_dataset', 'train', 'images')
    train_labels_path = os.path.join(output_base_path, 'main_dataset', 'train', 'labels')
    val_images_path = os.path.join(output_base_path, 'main_dataset', 'val', 'images')
    val_labels_path = os.path.join(output_base_path, 'main_dataset', 'val', 'labels')
    
    # Ensure directories exist
    for path in [train_images_path, train_labels_path, val_images_path, val_labels_path]:
        os.makedirs(path, exist_ok=True)
    
    # Get all FIR images
    image_files = [f for f in os.listdir(fir_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    processed_count = 0
    skipped_count = 0
    train_count = 0
    val_count = 0
    coordinate_errors = 0
    format_stats = {}
    size_stats = {'smaller': 0, 'larger': 0, 'exact': 0}
    
    for i, image_filename in enumerate(image_files):
        image_path = os.path.join(fir_folder, image_filename)
        
        # Find annotation
        annotation_path = find_ir_det_annotation(image_filename, ir_det_base_path)
        if annotation_path is None:
            print(f"Warning: No annotation found for {image_filename}")
            skipped_count += 1
            continue
        
        # Load original image to get ORIGINAL dimensions (CRITICAL!)
        original_image = cv2.imread(image_path)
        if original_image is None:
            print(f"Warning: Could not load image {image_path}")
            skipped_count += 1
            continue
        
        # Get ORIGINAL dimensions before any resizing
        original_height, original_width = original_image.shape[:2]
        
        # Track image size relative to target
        if original_width < target_size[0] or original_height < target_size[1]:
            size_stats['smaller'] += 1
        elif original_width > target_size[0] or original_height > target_size[1]:
            size_stats['larger'] += 1
        else:
            size_stats['exact'] += 1
        
        # Determine split (80% train, 20% val)
        is_train = i % 5 != 0  # Every 5th image goes to val
        
        # Create unique filename
        base_name = os.path.splitext(image_filename)[0]
        extension = os.path.splitext(image_filename)[1]
        unique_filename = f"ir_det_{base_name}{extension}"
        
        # Choose output paths based on split
        if is_train:
            output_image_path = os.path.join(train_images_path, unique_filename)
            output_label_path = os.path.join(train_labels_path, os.path.splitext(unique_filename)[0] + '.txt')
            train_count += 1
        else:
            output_image_path = os.path.join(val_images_path, unique_filename)
            output_label_path = os.path.join(val_labels_path, os.path.splitext(unique_filename)[0] + '.txt')
            val_count += 1
        
        # Process annotations with proper format detection
        yolo_annotations, format_type = parse_ir_det_annotations_CORRECTED(
            annotation_path, original_width, original_height, ir_det_to_yolo_classes
        )
        
        # Track format statistics
        format_stats[format_type] = format_stats.get(format_type, 0) + 1
        
        # Resize image with padding to maintain aspect ratio
        padded_image, transform_info = resize_image_with_padding(original_image, target_size)
        
        # Transform annotations to account for padding
        transformed_annotations = transform_yolo_annotations(yolo_annotations, transform_info, target_size)
        
        # Save padded image
        cv2.imwrite(output_image_path, padded_image)
        
        # Save YOLO format label file with transformed coordinates
        with open(output_label_path, 'w') as f:
            for annotation in transformed_annotations:
                class_id, x_center, y_center, width, height = annotation[:5]
                f.write(f"{int(class_id)} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
        
        processed_count += 1
        if processed_count % 100 == 0:
            print(f"  Processed {processed_count} images...")
    
    print(f"  Completed IR_det: {processed_count} images processed, {skipped_count} skipped")
    print(f"  Train: {train_count}, Val: {val_count}")
    print(f"  Image sizes: {size_stats['smaller']} smaller, {size_stats['larger']} larger, {size_stats['exact']} exact match")
    print(f"  Annotation formats detected: {format_stats}")

def update_dataset_yaml_with_ir_det(base_path):
    """Update the main dataset YAML file to include IR_det data"""
    yaml_path = os.path.join(base_path, 'main_dataset', 'data.yaml')
    
    # Updated class names including IR_det classes
    class_names = [
        'person',         # 0 (includes IR_det person, people, pedestrian, human)
        'bicycle',        # 1 (includes IR_det bicycle, bike)
        'car',           # 2 (includes IR_det car, vehicle, auto)
        'motorcycle',    # 3 (includes IR_det motorcycle, motorbike)
        'bus',           # 4
        'truck',         # 5 (includes IR_det truck, van)
        'dog',           # 6 (includes IR_det dog, animal)
        'skateboard',    # 7
        'other_vehicle'  # 8
    ]
    
    yaml_content = f"""# Combined Thermal Dataset Configuration (FLIR + SMOD + IR_det) - CORRECTED
path: {os.path.abspath(os.path.join(base_path, 'main_dataset'))}
train: train/images
val: val/images
test: test/images

# Number of classes
nc: {len(class_names)}

# Class names
names: {class_names}

# Sources: FLIR_ADAS_v2, SMOD, IR_det
# Processing: CORRECTED - Aspect ratio preserved with black padding
"""
    
    with open(yaml_path, 'w') as f:
        f.write(yaml_content)
    
    print(f"Updated data.yaml with IR_det classes at {yaml_path}")

def validate_ir_det_annotations(base_path, num_samples=5):
    """Validate IR_det annotations to ensure they look correct"""
    print(f"\nüîç VALIDATING IR_DET ANNOTATIONS")
    print("=" * 50)
    
    train_labels_path = os.path.join(base_path, 'main_dataset', 'train', 'labels')
    if not os.path.exists(train_labels_path):
        print("No training labels found for validation")
        return
    
    # Look specifically for IR_det files
    ir_det_files = [f for f in os.listdir(train_labels_path) if f.startswith('ir_det_')][:num_samples]
    
    if not ir_det_files:
        print("No IR_det files found for validation")
        return
    
    for label_file in ir_det_files:
        label_path = os.path.join(train_labels_path, label_file)
        print(f"\nüìÑ {label_file}:")
        
        try:
            with open(label_path, 'r') as f:
                lines = f.readlines()
            
            valid_count = 0
            invalid_count = 0
            class_distribution = {}
            
            for line in lines:
                parts = line.strip().split()
                if len(parts) >= 5:
                    try:
                        class_id = int(parts[0])
                        x, y, w, h = map(float, parts[1:5])
                        
                        # Track class distribution
                        class_distribution[class_id] = class_distribution.get(class_id, 0) + 1
                        
                        # Check if coordinates are in valid range
                        if (0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1):
                            valid_count += 1
                        else:
                            invalid_count += 1
                            print(f"   ‚ö†Ô∏è  Invalid: class={class_id}, x={x:.3f}, y={y:.3f}, w={w:.3f}, h={h:.3f}")
                    except ValueError:
                        invalid_count += 1
            
            print(f"   ‚úÖ Valid annotations: {valid_count}")
            print(f"   üéØ Classes found: {class_distribution}")
            if invalid_count > 0:
                print(f"   ‚ùå Invalid annotations: {invalid_count}")
            else:
                print(f"   üéØ All IR_det coordinates properly normalized!")
                
        except Exception as e:
            print(f"   Error reading file: {e}")

def main():
    # Set paths
    base_path = r"D:\datasets"
    ir_det_path = os.path.join(base_path, "IR_det")
    
    # Verify IR_det dataset exists
    if not os.path.exists(ir_det_path):
        print(f"Error: IR_det dataset not found at {ir_det_path}")
        return
    
    print("üöÄ STARTING CORRECTED IR_DET PREPROCESSING")
    print("=" * 60)
    print(f"Source: {ir_det_path}")
    print(f"Output: Adding to existing main_dataset")
    print("üñºÔ∏è IMAGE PROCESSING: Aspect ratio preserved with black padding")
    print("üîß CORRECTION: Proper annotation format detection and normalization")
    print("üå°Ô∏è  Using FIR (Far Infrared) images - closest to FLIR ADK")
    print("=" * 60)
    
    # Check if main_dataset exists
    main_dataset_path = os.path.join(base_path, 'main_dataset')
    if not os.path.exists(main_dataset_path):
        print("Warning: main_dataset folder doesn't exist. Run FLIR preprocessing first.")
        return
    
    # Process the dataset
    process_ir_det_data_CORRECTED(ir_det_path, base_path, target_size=(640, 512))
    
    # Update dataset configuration
    update_dataset_yaml_with_ir_det(base_path)
    
    # Validate sample annotations
    validate_ir_det_annotations(base_path)
    
    print("\n‚úÖ CORRECTED IR_det preprocessing completed!")
    print("üìä Summary:")
    print("   - 7,512 FIR thermal images with multispectral variety")
    print("   - Aspect ratio preserved with black padding")
    print("   - Annotations correctly transformed for padded images")
    print("   - Smart annotation format detection (YOLO vs pixel coordinates)")
    print("   - Proper coordinate normalization using original image dimensions")
    print("   - 80/20 train/val split for this dataset")
    print("\nReady for next dataset!")

if __name__ == "__main__":
    main()
