import os
import yaml
import shutil
from PIL import Image
import cv2
import numpy as np
from pathlib import Path

def get_complete_wildlife_class_mapping():
    """Get complete class mapping including all wildlife species"""
    # Complete class mapping with all wildlife
    unified_classes = {
        0: 'person',
        1: 'bicycle', 
        2: 'car',
        3: 'motorcycle',
        4: 'bus',
        5: 'truck',
        6: 'dog',
        7: 'deer',        # From previous deer datasets
        8: 'coyote',      # NEW from My Game Pics
        9: 'hog',         # NEW from My Game Pics
        10: 'rabbit',     # NEW from My Game Pics
        11: 'raccoon',    # NEW from My Game Pics
        12: 'skateboard', # Keep existing
        13: 'other_vehicle' # Keep existing
    }
    
    # Mapping from dataset-specific names to our unified classes
    wildlife_name_mapping = {
        # Deer (existing class)
        'deer': 7,
        'roe deer': 7,
        'roe_deer': 7,
        'white-tailed deer': 7,
        'whitetail': 7,
        'white_tailed_deer': 7,
        
        # New wildlife classes from My Game Pics
        'coyote': 8,
        'hog': 9,
        'pig': 9,
        'wild pig': 9,
        'wild_pig': 9,
        'boar': 9,
        'wild boar': 9,
        'wild_boar': 9,
        'rabbit': 10,
        'bunny': 10,
        'hare': 10,
        'raccoon': 11,
        'racoon': 11,  # Alternative spelling
    }
    
    return unified_classes, wildlife_name_mapping

def load_my_game_pics_class_names(dataset_path):
    """Load original class names from My Game Pics data.yaml"""
    data_yaml_path = os.path.join(dataset_path, 'data.yaml')
    original_class_names = []
    
    if os.path.exists(data_yaml_path):
        try:
            with open(data_yaml_path, 'r') as f:
                data_config = yaml.safe_load(f)
            original_class_names = data_config.get('names', [])
            print(f"📋 Original My Game Pics classes: {original_class_names}")
        except Exception as e:
            print(f"Warning: Could not read original data.yaml: {e}")
    
    return original_class_names

def resize_image_with_padding(image, target_size):
    """
    Resize image to target size while maintaining aspect ratio and padding with black.
    Returns resized image and transformation parameters.
    """
    target_width, target_height = target_size
    orig_height, orig_width = image.shape[:2]
    
    # Calculate scale factor to fit image within target size
    scale = min(target_width / orig_width, target_height / orig_height)
    
    # Calculate new dimensions
    new_width = int(orig_width * scale)
    new_height = int(orig_height * scale)
    
    # Resize image
    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
    
    # Create padded image
    padded = np.zeros((target_height, target_width, 3), dtype=np.uint8)
    
    # Calculate padding offsets (center the image)
    x_offset = (target_width - new_width) // 2
    y_offset = (target_height - new_height) // 2
    
    # Place resized image in center
    padded[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized
    
    # Return padded image and transformation info
    transform_info = {
        'scale': scale,
        'x_offset': x_offset,
        'y_offset': y_offset,
        'new_width': new_width,
        'new_height': new_height
    }
    
    return padded, transform_info

def transform_yolo_annotations(annotations, transform_info, target_size):
    """
    Transform YOLO annotations to account for padding and resizing.
    YOLO format: [class_id, x_center, y_center, width, height] (all normalized 0-1)
    """
    transformed_annotations = []
    target_width, target_height = target_size
    
    for ann in annotations:
        class_id, x_center, y_center, width, height = ann
        
        # Scale coordinates to padded image dimensions
        # Original coordinates are normalized (0-1), so we need to:
        # 1. Scale them to the resized (but not padded) dimensions
        # 2. Add the offset from padding
        # 3. Normalize back to 0-1 for the target size
        
        # Convert to pixel coordinates in resized image
        x_pixel = x_center * transform_info['new_width']
        y_pixel = y_center * transform_info['new_height']
        w_pixel = width * transform_info['new_width']
        h_pixel = height * transform_info['new_height']
        
        # Add padding offset
        x_pixel += transform_info['x_offset']
        y_pixel += transform_info['y_offset']
        
        # Convert back to normalized coordinates for target size
        x_norm = x_pixel / target_width
        y_norm = y_pixel / target_height
        w_norm = w_pixel / target_width
        h_norm = h_pixel / target_height
        
        # Ensure coordinates are within bounds
        x_norm = max(0.0, min(1.0, x_norm))
        y_norm = max(0.0, min(1.0, y_norm))
        w_norm = max(0.001, min(1.0, w_norm))
        h_norm = max(0.001, min(1.0, h_norm))
        
        transformed_annotations.append([class_id, x_norm, y_norm, w_norm, h_norm])
    
    return transformed_annotations

def validate_and_map_wildlife_label(label_file, original_class_names, wildlife_mapping):
    """Validate and map My Game Pics YOLO annotations to unified wildlife classes"""
    annotations = []
    
    try:
        with open(label_file, 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            parts = line.split()
            if len(parts) >= 5:
                try:
                    # YOLO format: class_id x_center y_center width height
                    original_class_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    
                    # Map original class to our unified wildlife system
                    unified_class_id = None
                    
                    if original_class_id < len(original_class_names):
                        class_name = original_class_names[original_class_id].lower()
                        
                        # Find unified class ID
                        unified_class_id = wildlife_mapping.get(class_name)
                        
                        if unified_class_id is None:
                            print(f"Warning: Unknown wildlife class '{class_name}' - skipping")
                            continue
                    else:
                        print(f"Warning: Class ID {original_class_id} out of range - skipping")
                        continue
                    
                    # Validate coordinates are in [0,1] range
                    if (0 <= x_center <= 1 and 0 <= y_center <= 1 and 
                        0 < width <= 1 and 0 < height <= 1):
                        annotations.append([unified_class_id, x_center, y_center, width, height])
                    else:
                        print(f"Warning: Invalid coordinates in line: {line}")
                    
                except ValueError:
                    print(f"Warning: Could not parse line: {line}")
                    continue
    
    except Exception as e:
        print(f"Warning: Error reading label file {label_file}: {e}")
    
    return annotations

def process_my_game_pics_split_CORRECTED(dataset_base_path, output_base_path, split_name, original_class_names, target_size=(640, 512)):
    """CORRECTED: Process My Game Pics data with multi-species wildlife mapping and proper aspect ratio"""
    
    # Map valid to val for consistency
    output_split = 'val' if split_name == 'valid' else split_name
    
    split_path = os.path.join(dataset_base_path, split_name)
    if not os.path.exists(split_path):
        print(f"Warning: Split {split_name} not found at {split_path}")
        return
    
    images_path = os.path.join(split_path, 'images')
    labels_path = os.path.join(split_path, 'labels')
    
    if not os.path.exists(images_path):
        print(f"Warning: Images folder not found at {images_path}")
        return
    
    if not os.path.exists(labels_path):
        print(f"Warning: Labels folder not found at {labels_path}")
        return
    
    print(f"Processing My Game Pics {split_name} split...")
    
    # Get class mappings
    unified_classes, wildlife_mapping = get_complete_wildlife_class_mapping()
    
    # Output paths - add to existing main_dataset
    output_images_path = os.path.join(output_base_path, 'main_dataset', output_split, 'images')
    output_labels_path = os.path.join(output_base_path, 'main_dataset', output_split, 'labels')
    
    # Ensure directories exist
    os.makedirs(output_images_path, exist_ok=True)
    os.makedirs(output_labels_path, exist_ok=True)
    
    # Get all images
    image_files = [f for f in os.listdir(images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    processed_count = 0
    skipped_count = 0
    coordinate_errors = 0
    class_counts = {}
    
    for image_filename in image_files:
        image_path = os.path.join(images_path, image_filename)
        
        # Find corresponding label file
        base_name = os.path.splitext(image_filename)[0]
        label_file = os.path.join(labels_path, base_name + '.txt')
        
        if not os.path.exists(label_file):
            print(f"Warning: No label found for {image_filename}")
            skipped_count += 1
            continue
        
        # Load original image
        original_image = cv2.imread(image_path)
        if original_image is None:
            print(f"Warning: Could not load image {image_path}")
            skipped_count += 1
            continue
        
        # Resize image with padding to maintain aspect ratio
        padded_image, transform_info = resize_image_with_padding(original_image, target_size)
        
        # Create unique filename
        clean_filename = os.path.basename(image_filename)
        base_name = os.path.splitext(clean_filename)[0]
        extension = os.path.splitext(clean_filename)[1]
        unique_filename = f"my_game_pics_{output_split}_{base_name}{extension}"
        
        # Save padded image
        output_image_path = os.path.join(output_images_path, unique_filename)
        cv2.imwrite(output_image_path, padded_image)
        
        # Parse and map existing YOLO annotations to unified wildlife classes
        original_annotations = validate_and_map_wildlife_label(label_file, original_class_names, wildlife_mapping)
        
        # Transform annotations to account for padding
        transformed_annotations = transform_yolo_annotations(original_annotations, transform_info, target_size)
        
        # Count classes for statistics
        for ann in transformed_annotations:
            class_id = ann[0]
            class_name = unified_classes.get(class_id, f"unknown_{class_id}")
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        if not transformed_annotations:
            coordinate_errors += 1
        
        # Save YOLO format label file with transformed coordinates
        label_filename = os.path.splitext(unique_filename)[0] + '.txt'
        label_path = os.path.join(output_labels_path, label_filename)
        
        with open(label_path, 'w') as f:
            for annotation in transformed_annotations:
                class_id, x_center, y_center, width, height = annotation
                f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
        
        processed_count += 1
        if processed_count % 50 == 0:
            print(f"  Processed {processed_count} images...")
    
    print(f"  Completed My Game Pics {split_name}: {processed_count} images processed, {skipped_count} skipped")
    print(f"  Wildlife distribution: {class_counts}")
    if coordinate_errors > 0:
        print(f"  Files with no valid annotations: {coordinate_errors}")

def update_dataset_yaml_with_complete_wildlife(base_path):
    """Update the main dataset YAML file to include complete wildlife classes"""
    yaml_path = os.path.join(base_path, 'main_dataset', 'data.yaml')
    
    # Complete class names including all wildlife
    class_names = [
        'person',         # 0
        'bicycle',        # 1  
        'car',           # 2
        'motorcycle',    # 3
        'bus',           # 4
        'truck',         # 5
        'dog',           # 6
        'deer',          # 7 (existing)
        'coyote',        # 8 (NEW)
        'hog',           # 9 (NEW)
        'rabbit',        # 10 (NEW)
        'raccoon',       # 11 (NEW)
        'skateboard',    # 12
        'other_vehicle'  # 13
    ]
    
    yaml_content = f"""# COMPLETE THERMAL + WILDLIFE DATASET CONFIGURATION - FINAL
path: {os.path.abspath(os.path.join(base_path, 'main_dataset'))}
train: train/images
val: val/images
test: test/images

# Number of classes
nc: {len(class_names)}

# Class names
names: {class_names}

# Sources: 
#   THERMAL: FLIR_ADAS_v2, SMOD, IR_det, KAIST, LLVIP, CVC14
#   WILDLIFE: Roe_deer, Szarvas, My_Game_Pics
# Processing: CORRECTED - Aspect ratio preserved with black padding
# Target: FLIR ADK 640x512 thermal detection system
"""
    
    with open(yaml_path, 'w') as f:
        f.write(yaml_content)
    
    print(f"Updated data.yaml with complete wildlife classes at {yaml_path}")

def validate_complete_wildlife_system(base_path):
    """Validate the complete wildlife detection system"""
    print(f"\n🦌 COMPLETE WILDLIFE DETECTION VALIDATION")
    print("=" * 60)
    
    train_labels_path = os.path.join(base_path, 'main_dataset', 'train', 'labels')
    if not os.path.exists(train_labels_path):
        print("No training labels found")
        return
    
    wildlife_stats = {
        'deer': {'files': 0, 'annotations': 0},      # class 7
        'coyote': {'files': 0, 'annotations': 0},    # class 8
        'hog': {'files': 0, 'annotations': 0},       # class 9
        'rabbit': {'files': 0, 'annotations': 0},    # class 10
        'raccoon': {'files': 0, 'annotations': 0},   # class 11
    }
    
    wildlife_class_map = {7: 'deer', 8: 'coyote', 9: 'hog', 10: 'rabbit', 11: 'raccoon'}
    
    # Count wildlife from all sources
    for label_file in os.listdir(train_labels_path):
        if any(label_file.startswith(prefix) for prefix in ['roe_deer_', 'szarvas_', 'my_game_pics_']):
            label_path = os.path.join(train_labels_path, label_file)
            
            try:
                with open(label_path, 'r') as f:
                    lines = f.readlines()
                
                file_wildlife = set()
                for line in lines:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        if class_id in wildlife_class_map:
                            wildlife_name = wildlife_class_map[class_id]
                            wildlife_stats[wildlife_name]['annotations'] += 1
                            file_wildlife.add(wildlife_name)
                
                # Count files containing each species
                for species in file_wildlife:
                    wildlife_stats[species]['files'] += 1
                    
            except Exception:
                continue
    
    print("🎯 WILDLIFE DETECTION CAPABILITIES:")
    for species, stats in wildlife_stats.items():
        print(f"   {species.upper()}: {stats['annotations']} annotations in {stats['files']} files")
    
    total_wildlife_annotations = sum(stats['annotations'] for stats in wildlife_stats.values())
    print(f"\n🦌 TOTAL WILDLIFE ANNOTATIONS: {total_wildlife_annotations}")
    print(f"🎯 WILDLIFE SPECIES SUPPORTED: {len([s for s in wildlife_stats if wildlife_stats[s]['annotations'] > 0])}")

def main():
    # Set paths
    base_path = r"D:\datasets"
    dataset_path = os.path.join(base_path, "My Game Pics.v7i.yolov11")
    
    # Verify dataset exists
    if not os.path.exists(dataset_path):
        print(f"Error: My Game Pics dataset not found at {dataset_path}")
        return
    
    print("🚀 STARTING MY GAME PICS PREPROCESSING (COMPLETE WILDLIFE)")
    print("=" * 70)
    print(f"Source: {dataset_path}")
    print(f"Output: Adding to existing main_dataset")
    print("🖼️ IMAGE PROCESSING: Aspect ratio preserved with black padding")
    print("🦌 WILDLIFE: Multi-species detection system")
    print("   🦌 deer → class 7 (enhance existing)")
    print("   🐺 coyote → class 8 (NEW)")
    print("   🐗 hog → class 9 (NEW)")
    print("   🐰 rabbit → class 10 (NEW)")
    print("   🦝 raccoon → class 11 (NEW)")
    print("=" * 70)
    
    # Load original class names
    original_class_names = load_my_game_pics_class_names(dataset_path)
    if not original_class_names:
        print("Error: Could not load original class names")
        return
    
    # Check if main_dataset exists
    main_dataset_path = os.path.join(base_path, 'main_dataset')
    if not os.path.exists(main_dataset_path):
        print("Warning: main_dataset folder doesn't exist. Run thermal datasets first.")
        return
    
    # Process train, valid, and test splits
    splits = ['train', 'valid', 'test']
    for split in splits:
        process_my_game_pics_split_CORRECTED(dataset_path, base_path, split, original_class_names, target_size=(640, 512))
    
    # Update the dataset YAML file with complete wildlife classes
    update_dataset_yaml_with_complete_wildlife(base_path)
    
    # Validate complete wildlife system
    validate_complete_wildlife_system(base_path)
    
    print("\n✅ MY GAME PICS preprocessing completed!")
    print("🎉 COMPLETE THERMAL + WILDLIFE DATASET READY!")
    print("📊 Summary:")
    print("   - Multi-species wildlife detection added")
    print("   - Aspect ratio preserved with black padding")
    print("   - Annotations correctly transformed for padded images")
    print("   - Complete hunting/game camera species coverage")
    print("   - Thermal + Wildlife = comprehensive outdoor detection")
    print("   - Ready for FLIR ADK deployment")
    print("\n🔥 ULTIMATE OUTDOOR DETECTION SYSTEM: COMPLETE! 🔥")

if __name__ == "__main__":
    main()
