import os
import shutil
from PIL import Image
import cv2
import numpy as np
from pathlib import Path
import glob

def resize_image_with_padding(image, target_size):
    """
    Resize image to target size while maintaining aspect ratio and padding with black.
    Works for both upscaling (small images) and downscaling (large images).
    Returns resized image and transformation parameters.
    """
    target_width, target_height = target_size
    orig_height, orig_width = image.shape[:2]
    
    # Calculate scale factor to fit image within target size
    scale = min(target_width / orig_width, target_height / orig_height)
    
    # Calculate new dimensions
    new_width = int(orig_width * scale)
    new_height = int(orig_height * scale)
    
    # Resize image (works for both upscaling and downscaling)
    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
    
    # Create padded image with black background
    padded = np.zeros((target_height, target_width, 3), dtype=np.uint8)
    
    # Calculate padding offsets (center the image)
    x_offset = (target_width - new_width) // 2
    y_offset = (target_height - new_height) // 2
    
    # Place resized image in center
    padded[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized
    
    # Return padded image and transformation info
    transform_info = {
        'scale': scale,
        'x_offset': x_offset,
        'y_offset': y_offset,
        'new_width': new_width,
        'new_height': new_height
    }
    
    return padded, transform_info

def transform_yolo_annotations(annotations, transform_info, target_size):
    """
    Transform YOLO annotations to account for padding and resizing.
    Input annotations are already normalized [0,1] based on original image.
    """
    transformed_annotations = []
    target_width, target_height = target_size
    
    for ann in annotations:
        class_id, x_center, y_center, width, height = ann
        
        # Convert normalized coordinates to pixel coordinates in resized image
        x_pixel = x_center * transform_info['new_width']
        y_pixel = y_center * transform_info['new_height']
        w_pixel = width * transform_info['new_width']
        h_pixel = height * transform_info['new_height']
        
        # Add padding offset
        x_pixel += transform_info['x_offset']
        y_pixel += transform_info['y_offset']
        
        # Convert back to normalized coordinates for target size
        x_norm = x_pixel / target_width
        y_norm = y_pixel / target_height
        w_norm = w_pixel / target_width
        h_norm = h_pixel / target_height
        
        # Ensure coordinates are within bounds
        x_norm = max(0.0, min(1.0, x_norm))
        y_norm = max(0.0, min(1.0, y_norm))
        w_norm = max(0.001, min(1.0, w_norm))
        h_norm = max(0.001, min(1.0, h_norm))
        
        transformed_annotations.append([class_id, x_norm, y_norm, w_norm, h_norm])
    
    return transformed_annotations

def parse_cvc14_annotation_CORRECTED(annotation_file, original_width, original_height):
    """CORRECTED: Parse CVC14 annotation file with proper format detection and conversion"""
    annotations = []
    
    try:
        with open(annotation_file, 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):  # Skip empty lines and comments
                continue
            
            parts = line.split()
            if len(parts) >= 4:
                try:
                    # CVC14 annotations can be in different formats:
                    # Format 1: x y w h (top-left corner + dimensions)
                    # Format 2: x1 y1 x2 y2 (corners)  
                    # Format 3: class x y w h (with class)
                    
                    if len(parts) == 4:
                        # Assume x y w h format (pedestrian detection)
                        x, y, w, h = map(float, parts)
                        
                        # Convert from top-left + dimensions to center + dimensions
                        x_center = x + w / 2
                        y_center = y + h / 2
                        
                        # Normalize using ORIGINAL image dimensions (CRITICAL!)
                        x_center_norm = x_center / original_width
                        y_center_norm = y_center / original_height
                        width_norm = w / original_width
                        height_norm = h / original_height
                        
                        # Validate (don't clamp yet - let transform handle it)
                        if (0 <= x_center_norm <= 1 and 0 <= y_center_norm <= 1 and 
                            0 < width_norm <= 1 and 0 < height_norm <= 1):
                            annotations.append({
                                'class': 'person',  # CVC14 is pedestrian-focused
                                'bbox': [x_center_norm, y_center_norm, width_norm, height_norm]
                            })
                        
                    elif len(parts) == 5:
                        # Could be class x y w h or x1 y1 x2 y2 + confidence
                        if parts[0].isdigit():
                            # class x y w h format
                            class_id, x, y, w, h = parts
                            x, y, w, h = map(float, [x, y, w, h])
                            
                            # Convert to center coordinates
                            x_center = x + w / 2
                            y_center = y + h / 2
                            
                            # Normalize using ORIGINAL dimensions
                            x_center_norm = x_center / original_width
                            y_center_norm = y_center / original_height
                            width_norm = w / original_width
                            height_norm = h / original_height
                            
                            # Validate
                            if (0 <= x_center_norm <= 1 and 0 <= y_center_norm <= 1 and 
                                0 < width_norm <= 1 and 0 < height_norm <= 1):
                                annotations.append({
                                    'class': 'person',
                                    'bbox': [x_center_norm, y_center_norm, width_norm, height_norm]
                                })
                        else:
                            # x1 y1 x2 y2 confidence format
                            x1, y1, x2, y2, conf = map(float, parts)
                            
                            # Convert corners to center + dimensions
                            x_center = (x1 + x2) / 2
                            y_center = (y1 + y2) / 2
                            width = x2 - x1
                            height = y2 - y1
                            
                            # Normalize using ORIGINAL dimensions
                            x_center_norm = x_center / original_width
                            y_center_norm = y_center / original_height
                            width_norm = width / original_width
                            height_norm = height / original_height
                            
                            # Validate
                            if (0 <= x_center_norm <= 1 and 0 <= y_center_norm <= 1 and 
                                0 < width_norm <= 1 and 0 < height_norm <= 1):
                                annotations.append({
                                    'class': 'person',
                                    'bbox': [x_center_norm, y_center_norm, width_norm, height_norm]
                                })
                    
                except ValueError:
                    # Skip lines that can't be parsed as numbers
                    continue
    
    except Exception as e:
        print(f"Warning: Error parsing annotation file {annotation_file}: {e}")
    
    return annotations

def get_cvc14_class_mapping():
    """Map CVC14 classes to our unified YOLO classes"""
    cvc14_to_yolo_classes = {
        'person': 0,
        'pedestrian': 0,
        'people': 0,
        'human': 0,
    }
    
    return cvc14_to_yolo_classes

def process_cvc14_split_CORRECTED(cvc14_base_path, output_base_path, time_period, split_type, target_size=(640, 512)):
    """CORRECTED: Process CVC14 data with proper bbox normalization and aspect ratio preservation"""
    
    # Use FIR (thermal) images
    fir_path = os.path.join(cvc14_base_path, time_period, 'FIR', split_type)
    
    if not os.path.exists(fir_path):
        print(f"Warning: FIR path not found at {fir_path}")
        return
    
    print(f"Processing CVC14 {time_period}/{split_type} (FIR thermal images)...")
    
    # Look for image and annotation folders
    frames_path = os.path.join(fir_path, 'FramesPos')
    annotations_path = os.path.join(fir_path, 'Annotations')
    
    if not os.path.exists(frames_path):
        print(f"Warning: FramesPos not found at {frames_path}")
        return
    
    if not os.path.exists(annotations_path):
        print(f"Warning: Annotations not found at {annotations_path}")
        return
    
    # Get class mapping
    cvc14_to_yolo_classes = get_cvc14_class_mapping()
    
    # Determine output split (Train->train, NewTest->val)
    output_split = 'train' if split_type == 'Train' else 'val'
    
    # Output paths - add to existing main_dataset
    output_images_path = os.path.join(output_base_path, 'main_dataset', output_split, 'images')
    output_labels_path = os.path.join(output_base_path, 'main_dataset', output_split, 'labels')
    
    # Ensure directories exist
    os.makedirs(output_images_path, exist_ok=True)
    os.makedirs(output_labels_path, exist_ok=True)
    
    # Get all TIF images
    image_files = [f for f in os.listdir(frames_path) if f.lower().endswith('.tif')]
    
    processed_count = 0
    skipped_count = 0
    coordinate_errors = 0
    size_stats = {'smaller': 0, 'larger': 0, 'exact': 0}
    
    for image_filename in image_files:
        image_path = os.path.join(frames_path, image_filename)
        
        # Find corresponding annotation file
        base_name = os.path.splitext(image_filename)[0]
        annotation_file = os.path.join(annotations_path, base_name + '.txt')
        
        if not os.path.exists(annotation_file):
            # Try alternative annotation naming
            alt_annotation_file = os.path.join(annotations_path, image_filename.replace('.tif', '.txt'))
            if os.path.exists(alt_annotation_file):
                annotation_file = alt_annotation_file
            else:
                print(f"Warning: No annotation found for {image_filename}")
                skipped_count += 1
                continue
        
        # Load original image to get ORIGINAL dimensions (CRITICAL!)
        original_image = cv2.imread(image_path)
        if original_image is None:
            print(f"Warning: Could not load image {image_path}")
            skipped_count += 1
            continue
        
        # Get ORIGINAL dimensions before any resizing
        original_height, original_width = original_image.shape[:2]
        
        # Track image size relative to target
        if original_width < target_size[0] or original_height < target_size[1]:
            size_stats['smaller'] += 1
        elif original_width > target_size[0] or original_height > target_size[1]:
            size_stats['larger'] += 1
        else:
            size_stats['exact'] += 1
        
        # Create unique filename - convert to JPG for consistency
        clean_filename = os.path.basename(image_filename)
        base_name = os.path.splitext(clean_filename)[0]
        unique_filename = f"cvc14_{time_period.lower()}_{output_split}_{base_name}.jpg"
        
        # Parse annotations using ORIGINAL dimensions
        annotations = parse_cvc14_annotation_CORRECTED(annotation_file, original_width, original_height)
        
        # Process annotations
        yolo_annotations = []
        for ann in annotations:
            class_name = ann['class'].lower()
            if class_name in cvc14_to_yolo_classes:
                yolo_class_id = cvc14_to_yolo_classes[class_name]
                
                # Get normalized bbox (already converted in parser)
                x_center, y_center, width, height = ann['bbox']
                yolo_annotations.append([yolo_class_id, x_center, y_center, width, height])
        
        # Resize image with padding to maintain aspect ratio
        padded_image, transform_info = resize_image_with_padding(original_image, target_size)
        
        # Transform annotations to account for padding
        transformed_annotations = transform_yolo_annotations(yolo_annotations, transform_info, target_size)
        
        # Save padded image as JPG
        output_image_path = os.path.join(output_images_path, unique_filename)
        cv2.imwrite(output_image_path, padded_image)
        
        # Save YOLO format label file with transformed coordinates
        label_filename = os.path.splitext(unique_filename)[0] + '.txt'
        label_path = os.path.join(output_labels_path, label_filename)
        
        with open(label_path, 'w') as f:
            for annotation in transformed_annotations:
                class_id, x_center, y_center, width, height = annotation
                f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
        
        processed_count += 1
        if processed_count % 50 == 0:
            print(f"  Processed {processed_count} images...")
    
    print(f"  Completed CVC14 {time_period}/{split_type}: {processed_count} images processed, {skipped_count} skipped")
    print(f"  Image sizes: {size_stats['smaller']} smaller, {size_stats['larger']} larger, {size_stats['exact']} exact match")
    if coordinate_errors > 0:
        print(f"  Warning: {coordinate_errors} coordinate errors found")

def update_dataset_yaml_with_cvc14(base_path):
    """Update the main dataset YAML file to include CVC14 data"""
    yaml_path = os.path.join(base_path, 'main_dataset', 'data.yaml')
    
    # Updated class names including CVC14 classes (same as before, CVC14 is person-focused)
    class_names = [
        'person',         # 0 (includes CVC14 person, pedestrian)
        'bicycle',        # 1
        'car',           # 2
        'motorcycle',    # 3
        'bus',           # 4
        'truck',         # 5
        'dog',           # 6
        'skateboard',    # 7
        'other_vehicle'  # 8
    ]
    
    yaml_content = f"""# Combined Thermal Dataset Configuration (FLIR + SMOD + IR_det + KAIST + LLVIP + CVC14) - CORRECTED
path: {os.path.abspath(os.path.join(base_path, 'main_dataset'))}
train: train/images
val: val/images
test: test/images

# Number of classes
nc: {len(class_names)}

# Class names
names: {class_names}

# Sources: FLIR_ADAS_v2, SMOD, IR_det, KAIST, LLVIP, CVC14
# Processing: CORRECTED - Aspect ratio preserved with black padding
"""
    
    with open(yaml_path, 'w') as f:
        f.write(yaml_content)
    
    print(f"Updated data.yaml with CVC14 classes at {yaml_path}")

def validate_cvc14_annotations(base_path, num_samples=5):
    """Validate CVC14 annotations to ensure they look correct"""
    print(f"\n🔍 VALIDATING CVC14 ANNOTATIONS")
    print("=" * 50)
    
    train_labels_path = os.path.join(base_path, 'main_dataset', 'train', 'labels')
    if not os.path.exists(train_labels_path):
        print("No training labels found for validation")
        return
    
    # Look specifically for CVC14 files
    cvc14_files = [f for f in os.listdir(train_labels_path) if f.startswith('cvc14_')][:num_samples]
    
    if not cvc14_files:
        print("No CVC14 files found for validation")
        return
    
    for label_file in cvc14_files:
        label_path = os.path.join(train_labels_path, label_file)
        print(f"\n📄 {label_file}:")
        
        try:
            with open(label_path, 'r') as f:
                lines = f.readlines()
            
            valid_count = 0
            invalid_count = 0
            class_distribution = {}
            
            for line in lines:
                parts = line.strip().split()
                if len(parts) >= 5:
                    try:
                        class_id = int(parts[0])
                        x, y, w, h = map(float, parts[1:5])
                        
                        # Track class distribution
                        class_distribution[class_id] = class_distribution.get(class_id, 0) + 1
                        
                        # Check if coordinates are in valid range
                        if (0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1):
                            valid_count += 1
                        else:
                            invalid_count += 1
                            print(f"   ⚠️  Invalid: class={class_id}, x={x:.3f}, y={y:.3f}, w={w:.3f}, h={h:.3f}")
                    except ValueError:
                        invalid_count += 1
            
            print(f"   ✅ Valid annotations: {valid_count}")
            print(f"   🎯 Classes found: {class_distribution}")
            if invalid_count > 0:
                print(f"   ❌ Invalid annotations: {invalid_count}")
            else:
                print(f"   🎯 All CVC14 coordinates properly normalized!")
                
        except Exception as e:
            print(f"   Error reading file: {e}")

def main():
    # Set paths
    base_path = r"D:\datasets"
    cvc14_path = os.path.join(base_path, "CVC14")
    
    # Verify CVC14 dataset exists
    if not os.path.exists(cvc14_path):
        print(f"Error: CVC14 dataset not found at {cvc14_path}")
        return
    
    print("🚀 STARTING CORRECTED CVC14 PREPROCESSING")
    print("=" * 60)
    print(f"Source: {cvc14_path}")
    print(f"Output: Adding to existing main_dataset")
    print("🖼️ IMAGE PROCESSING: Aspect ratio preserved with black padding")
    print("🔧 CORRECTION: Using original image dimensions for bbox normalization")
    print("🌡️  Using FIR thermal images - day/night pedestrian detection")
    print("🚶 Focus: Dense pedestrian annotations in challenging scenarios")
    print("🌅🌙 Day + Night scenarios for robust training")
    print("=" * 60)
    
    # Check if main_dataset exists
    main_dataset_path = os.path.join(base_path, 'main_dataset')
    if not os.path.exists(main_dataset_path):
        print("Warning: main_dataset folder doesn't exist. Run FLIR preprocessing first.")
        return
    
    # Process Day and Night, Train and NewTest splits
    time_periods = ['Day', 'Night']
    split_types = ['Train', 'NewTest']
    
    for time_period in time_periods:
        for split_type in split_types:
            process_cvc14_split_CORRECTED(cvc14_path, base_path, time_period, split_type, target_size=(640, 512))
    
    # Update dataset configuration
    update_dataset_yaml_with_cvc14(base_path)
    
    # Validate sample annotations
    validate_cvc14_annotations(base_path)
    
    print("\n✅ CORRECTED CVC14 preprocessing completed!")
    print("📊 Summary:")
    print("   - ~8,000+ FIR thermal pedestrian images")
    print("   - Day/Night scenarios for robust lighting conditions")
    print("   - Dense pedestrian annotations (~1,500+ per sequence)")
    print("   - TIF images converted to JPG for consistency")
    print("   - Aspect ratio preserved with black padding")
    print("   - Annotations correctly transformed for padded images")
    print("   - Bounding boxes normalized using ORIGINAL image dimensions")
    print("   - All coordinates should now be in valid [0,1] range")
    print("\nReady for wildlife datasets!")

if __name__ == "__main__":
    main()
